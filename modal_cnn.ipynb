{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ7MP8puTLgX"
      },
      "source": [
        "#Install and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmP9Ez5TTC6O"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b56uqiuFTKMA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tqdm\n",
        "import torch\n",
        "import random\n",
        "import logging\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from shutil import copyfile\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.model_selection import KFold\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKGlPN0UTSRQ"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyYR6iHvTVKX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jutrera/stanford-car-dataset-by-classes-folder\n",
        "\n",
        "with zipfile.ZipFile(\"stanford-car-dataset-by-classes-folder.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"stanford_cars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m0pcJbuT87i"
      },
      "source": [
        "# Working on the meta data of an image, decomposing the name of the image, and connecting to the correct classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z07x-UTsUNiU"
      },
      "outputs": [],
      "source": [
        "train_car = glob.glob(\"/content/stanford_cars/car_data/car_data/train/*/*\")\n",
        "test_car = glob.glob(\"/content/stanford_cars/car_data/car_data/test/*/*\")\n",
        "def get_car_class(car):\n",
        "    '''\n",
        "    This function extracts the car label/class per given image.\n",
        "    Additionally, it decomposes the class into manufacturer, type, year,\n",
        "    and includes both image number and full image filename.\n",
        "    '''\n",
        "\n",
        "    # Extract the car class from the folder name\n",
        "    car_class = car.split('/')[-2]  # This gets the folder name containing the class label\n",
        "\n",
        "    # Extract image filename and image number\n",
        "    image_file = car.split('/')[-1]  # Full image filename with extension\n",
        "    image_number = image_file.split('.')[0]  # Extracts the file name without extension\n",
        "\n",
        "    # Parse the class into details\n",
        "    car_parts = car_class.split()\n",
        "    manufacturer = car_parts[0]\n",
        "    car_type = \" \".join(car_parts[1:-1])\n",
        "    year = car_parts[-1]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"full_name\": car_class,\n",
        "        \"manufacturer\": manufacturer,\n",
        "        \"type\": car_type,\n",
        "        \"year\": year,\n",
        "        \"image_number\": image_number,\n",
        "        \"image_name\": image_file\n",
        "    }\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "for i in range(len(train_car)):\n",
        "  y_train.append(get_car_class(train_car[i]))\n",
        "\n",
        "## converting each photo into a numpy array of RGB pixels\n",
        "for i in range(len(test_car)):\n",
        "    y_test.append(get_car_class(test_car[i]))\n",
        "\n",
        "df_train = pd.DataFrame(y_train)\n",
        "\n",
        "df_test = pd.DataFrame(y_test)\n",
        "\n",
        "df_train.to_csv(\"train_data.csv\", index=False)\n",
        "df_test.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "print(\"Train and Test data saved as CSV files.\")\n",
        "df_train_labels = pd.read_csv('/content/train_data.csv')\n",
        "df_test_labels = pd.read_csv('/content/test_data.csv')\n",
        "\n",
        "columns = [\"image_name\", \"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\"]\n",
        "train_annotations = pd.read_csv('/content/stanford_cars/anno_train.csv', header=None, names=columns)\n",
        "test_annotations = pd.read_csv('/content/stanford_cars/anno_test.csv', header=None, names=columns)\n",
        "\n",
        "merged_data_train = pd.merge(df_train_labels, train_annotations, on='image_name', how='inner')\n",
        "merged_data_test = pd.merge(df_test_labels, test_annotations, on='image_name', how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Car dataset"
      ],
      "metadata": {
        "id": "MQe1VAtTGB3Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiy36oVcTkTP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CarDataset\n",
        "\n",
        "This class is designed to manage and load image data from the Stanford Cars Dataset efficiently,\n",
        "allowing seamless integration with PyTorch for machine learning tasks. It provides tools for\n",
        "loading, preprocessing, and interacting with the dataset, including mapping class names to IDs,\n",
        "retrieving images, and performing image transformations.\n",
        "\n",
        "Methods:\n",
        "--------\n",
        "__init__(annotations, root_dir, transform=None):\n",
        "    Initializes the dataset with metadata, root directory, and optional transformations.\n",
        "    Creates mappings between car types and their respective class IDs.\n",
        "\n",
        "__len__():\n",
        "    Returns the total number of images in the dataset.\n",
        "\n",
        "__getitem__(idx):\n",
        "    Retrieves the image and its metadata for a given index, applies transformations, and\n",
        "    crops the image if bounding box data is available.\n",
        "\n",
        "_generate_samples():\n",
        "    Generates a list of tuples (image_path, class_id) for all images in the dataset.\n",
        "\n",
        "generate_car_indax_image_dict():\n",
        "    Creates a dictionary mapping each class ID to a list of image paths belonging to that class.\n",
        "\n",
        "generate_car_image_dict():\n",
        "    Creates a dictionary mapping each car type (full name) to a list of image paths belonging to it.\n",
        "\n",
        "display_image_from_class(class_id):\n",
        "    Displays an image from the given car class using the class ID. The first available image is shown.\n",
        "\n",
        "print_number_of_images_in_class(class_id):\n",
        "    Prints the number of images belonging to the specified car class based on its class ID.\n",
        "\n",
        "crop_image_by_class_and_index(class_id, image_index):\n",
        "    Crops a specific image from a given car class and index using its bounding box coordinates.\n",
        "\n",
        "Properties:\n",
        "-----------\n",
        "classes:\n",
        "    Returns a list of all car type names (class names) in the dataset.\n",
        "\n",
        "class_to_idx:\n",
        "    Returns a dictionary mapping car type names to their corresponding class IDs.\n",
        "\n",
        "samples:\n",
        "    Returns a list of tuples (image_path, class_id) for all images in the dataset.\n",
        "\n",
        "targets:\n",
        "    Returns a list of class IDs corresponding to each image in the dataset.\n",
        "\n",
        "Usage:\n",
        "------\n",
        "This class is ideal for:\n",
        "1. Loading image data and associated metadata from the Stanford Cars Dataset.\n",
        "2. Preprocessing images, including cropping and applying transformations.\n",
        "3. Managing dataset annotations for use in machine learning pipelines.\n",
        "4. Integrating with PyTorch's DataLoader for efficient data batching and loading.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CarDataset(Dataset):\n",
        "    def __init__(self, annotations, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        annotations: The merged table containing all metadata about the images.\n",
        "        root_dir: The base directory where the images are stored.\n",
        "        transform: Transformations (if required).\n",
        "        \"\"\"\n",
        "        self.annotations = annotations\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Generate the car type-to-id mapping\n",
        "        self.class_to_idx1 = dict(zip(self.annotations['full_name'], self.annotations['class_id']))\n",
        "        self.idx_to_class = {idx: car_type for car_type, idx in self.class_to_idx1.items()}\n",
        "        self.samples1 = self._generate_samples()\n",
        "        self.targets1 = [item[1] for item in self.samples]  # Class ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image details from the metadata\n",
        "        row = self.annotations.iloc[idx]\n",
        "        image_name = row['image_name']\n",
        "        class_id = row['class_id']\n",
        "        manufacturer = row['manufacturer']\n",
        "        car_type = row['type']\n",
        "        year = row['year']\n",
        "        full_name = row['full_name']\n",
        "        bbox = (row['x_min'], row['y_min'], row['x_max'], row['y_max'])\n",
        "\n",
        "        # Build the full image path\n",
        "        img_path = os.path.join(self.root_dir, row['full_name'], row['image_name'])\n",
        "        # Open the image\n",
        "        image = Image.open(img_path)\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        # Crop the image based on the bounding box (if provided)\n",
        "        if bbox:\n",
        "            image = image.crop(bbox)\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, class_id\n",
        "\n",
        "    def _generate_samples(self):\n",
        "        \"\"\"\n",
        "        Helper function to generate a list of (image_path, class_id) tuples.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        for _, row in self.annotations.iterrows():\n",
        "            img_name = row['image_name']\n",
        "            img_path = os.path.join(self.root_dir, row['full_name'], img_name)\n",
        "            class_id = row['class_id']\n",
        "            samples.append((img_path, class_id))\n",
        "        return samples\n",
        "\n",
        "\n",
        "    def generate_car_indax_image_dict(self):\n",
        "        car_image_dict = {}\n",
        "\n",
        "        # Iterate through all annotations and group images by car type\n",
        "        for _, row in self.annotations.iterrows():\n",
        "            class_id = row['class_id']\n",
        "            car_type = row['full_name']  # Using the car type from the merged metadata\n",
        "\n",
        "            img_name = row['image_name']\n",
        "            img_path = os.path.join(self.root_dir, row['full_name'], img_name)\n",
        "\n",
        "            if class_id not in car_image_dict:\n",
        "                car_image_dict[class_id] = []\n",
        "            car_image_dict[class_id].append(img_path)\n",
        "        return car_image_dict\n",
        "\n",
        "\n",
        "\n",
        "    def generate_car_image_dict(self):\n",
        "        \"\"\"\n",
        "        Generate a dictionary of car types and the images that belong to them.\n",
        "        The dictionary will be of the form:\n",
        "        {car_type_name: [image_path1, image_path2, ...]}\n",
        "        \"\"\"\n",
        "        car_image_dict = {}\n",
        "\n",
        "        # Iterate through all annotations and group images by car type\n",
        "        for _, row in self.annotations.iterrows():\n",
        "            class_id = row['class_id']\n",
        "            car_type = row['full_name']  # Using the car type from the merged metadata\n",
        "\n",
        "            img_name = row['image_name']\n",
        "            img_path = os.path.join(self.root_dir, row['full_name'], img_name)\n",
        "\n",
        "            if car_type not in car_image_dict:\n",
        "                car_image_dict[car_type] = []\n",
        "            car_image_dict[car_type].append(img_path)\n",
        "        return car_image_dict\n",
        "\n",
        "    def display_image_from_class(self, class_id):\n",
        "        \"\"\"\n",
        "        Given a class_id, this function displays one image from that car class.\n",
        "        \"\"\"\n",
        "        # Get car type from class_id\n",
        "        car_type = self.idx_to_class.get(class_id, None)\n",
        "\n",
        "        if not car_type:\n",
        "            print(f\"Unknown class_id: {class_id}\")\n",
        "            return\n",
        "\n",
        "        # Retrieve all images for the given car class\n",
        "        car_image_paths = self.generate_car_image_dict().get(car_type, [])\n",
        "\n",
        "        if not car_image_paths:\n",
        "            print(f\"No images found for class {car_type}\")\n",
        "            return\n",
        "\n",
        "        # Select the first image from the list\n",
        "        img_path = car_image_paths[0]\n",
        "\n",
        "        try:\n",
        "            # Open and display the image\n",
        "            image = Image.open(img_path)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')  # Hide axes\n",
        "            plt.title(f\"Class: {car_type}\", fontsize=10)\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening image: {img_path}, Error: {e}\")\n",
        "\n",
        "    def print_number_of_images_in_class(self, class_id):\n",
        "        \"\"\"\n",
        "        Given a class_id, this function prints the number of images in that car class.\n",
        "        \"\"\"\n",
        "        # Get car type from class_id\n",
        "        car_type = self.idx_to_class.get(class_id, None)\n",
        "\n",
        "        if not car_type:\n",
        "            print(f\"Unknown class_id: {class_id}\")\n",
        "            return\n",
        "        # Retrieve all images for the given car class\n",
        "        car_image_paths = self.generate_car_image_dict().get(car_type, [])\n",
        "        # Print the number of images for the car type\n",
        "        print(f\"Number of images for class {car_type}: {len(car_image_paths)}\")\n",
        "\n",
        "    def crop_image_by_class_and_index(self, class_id, image_index):\n",
        "        \"\"\"\n",
        "        Crop an image based on class_id and image_index.\n",
        "        The image will be cropped using the bounding box coordinates (x_min, y_min, x_max, y_max).\n",
        "        \"\"\"\n",
        "        # Filter annotations for the given class_id\n",
        "        class_annotations = self.annotations[self.annotations['class_id'] == class_id]\n",
        "\n",
        "        # Ensure that the image_index is within the range\n",
        "        if image_index >= len(class_annotations):\n",
        "            print(f\"Invalid image index: {image_index} for class_id {class_id}\")\n",
        "            return None\n",
        "\n",
        "        # Get the annotation for the given image index\n",
        "        row = class_annotations.iloc[image_index]\n",
        "        image_name = row[\"image_name\"]\n",
        "        bbox_xmin = row[\"x_min\"]\n",
        "        bbox_ymin = row[\"y_min\"]\n",
        "        bbox_xmax = row[\"x_max\"]\n",
        "        bbox_ymax = row[\"y_max\"]\n",
        "\n",
        "        # Find the image path and car model name\n",
        "        image_path = os.path.join(self.root_dir, row['full_name'], image_name)\n",
        "        if os.path.exists(image_path):\n",
        "            try:\n",
        "                # Open and crop the image\n",
        "                image = Image.open(image_path)\n",
        "                cropped_image = image.crop((bbox_xmin, bbox_ymin, bbox_xmax, bbox_ymax))\n",
        "                return cropped_image\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_name}: {e}\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"Image not found: {image_name}\")\n",
        "            return None\n",
        "\n",
        "    # Methods added for functionality similar to ImageFolder:\n",
        "    @property\n",
        "    def classes(self):\n",
        "        \"\"\"Returns the list of car types (class names).\"\"\"\n",
        "        return list(self.idx_to_class.values())\n",
        "\n",
        "    @property\n",
        "    def class_to_idx(self):\n",
        "        \"\"\"Returns a dictionary of class names to class indices.\"\"\"\n",
        "        return self.class_to_idx1\n",
        "\n",
        "    @property\n",
        "    def samples(self):\n",
        "        \"\"\"Returns a list of (image_path, class_id) tuples.\"\"\"\n",
        "        return self.samples1\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        \"\"\"Returns a list of class_ids (targets) for each image.\"\"\"\n",
        "        return self.targets1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZxJ60XPVLmq"
      },
      "source": [
        "##Tests for the cardataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg18uMKTVQc3"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize images to 128x128 for consistency\n",
        "    transforms.ToTensor(),          # Convert images to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
        "])\n",
        "train_data = CarDataset(merged_data_train, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "type(train_data.__getitem__(1))\n",
        "tensor,labl=train_data.__getitem__(1)\n",
        "\n",
        "# Reverse normalization (assuming the normalization was done with mean=[0,0,0] and std=[1,1,1])\n",
        "tensor = tensor * 0.5 + 0.5  # Scale back to [0, 1] from [-1, 1]\n",
        "\n",
        "# Convert tensor to numpy array\n",
        "image = tensor.permute(1, 2, 0).numpy()  # Change shape to [H, W, C] for displaying\n",
        "print(labl)\n",
        "# Plot the image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # Hide axis for better visualization\n",
        "plt.show()\n",
        "train_data.class_to_idx['Acura Integra Type R 2001']\n",
        "train_data.classes\n",
        "train_data.samples\n",
        "train_data.targets\n",
        "train_data.__len__()\n",
        "train_data.crop_image_by_class_and_index(1,2)\n",
        "train_data.display_image_from_class(6)\n",
        "train_data.print_number_of_images_in_class(2)\n",
        "train_data.generate_car_image_dict()\n",
        "train_data._generate_samples()\n",
        "train_data.display_image_from_class(66)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3uKRHosXOkO"
      },
      "source": [
        "#DATALODAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggwxZSx_XQgO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CarDataLoader\n",
        "\n",
        "A custom DataLoader designed to work with the CarDataset or any compatible PyTorch Dataset.\n",
        "This class provides batch loading, shuffling, and other advanced options for dataset management,\n",
        "making it suitable for machine learning tasks.\n",
        "\n",
        "Initialization Parameters:\n",
        "--------------------------\n",
        "dataset: Dataset\n",
        "    The dataset from which to load the data. Should be compatible with the CarDataset class.\n",
        "batch_size: int, optional (default=32)\n",
        "    The number of samples per batch to load.\n",
        "shuffle: bool, optional (default=True)\n",
        "    Whether to shuffle the data at every epoch.\n",
        "num_workers: int, optional (default=0)\n",
        "    The number of subprocesses to use for data loading. If 0, data will be loaded in the main process.\n",
        "collate_fn: callable, optional (default=None)\n",
        "    A function to merge a list of samples into a batch. If None, a default function is used.\n",
        "sampler: iterable, optional (default=None)\n",
        "    A custom sampler that specifies the order in which data is loaded. If None, the default sampler is used.\n",
        "drop_last: bool, optional (default=False)\n",
        "    If True, drops the last incomplete batch.\n",
        "pin_memory: bool, optional (default=False)\n",
        "    If True, copies Tensors into CUDA pinned memory before returning them.\n",
        "timeout: float, optional (default=0)\n",
        "    The timeout value for collecting a batch.\n",
        "worker_init_fn: callable, optional (default=None)\n",
        "    A function to initialize each worker process.\n",
        "prefetch_factor: int, optional (default=None)\n",
        "    Number of samples loaded in advance by each worker.\n",
        "persistent_workers: bool, optional (default=False)\n",
        "    Whether to keep worker processes alive after the iterator is exhausted.\n",
        "pin_memory_device: str, optional (default='')\n",
        "    Device to pin memory for data transfer (e.g., \"cuda\").\n",
        "\n",
        "Methods:\n",
        "--------\n",
        "__len__():\n",
        "    Returns the number of batches in the dataset. Handles rounding for incomplete batches based on drop_last.\n",
        "\n",
        "__iter__():\n",
        "    Returns an iterator over batches of data. If shuffling is enabled, data indices are shuffled at the start.\n",
        "\n",
        "default_collate_fn(batch_data):\n",
        "    The default collate function that stacks images and labels into batches.\n",
        "    Converts images into a stacked tensor and labels into a tensor. Handles pinning memory for CUDA if enabled.\n",
        "\n",
        "Attributes:\n",
        "-----------\n",
        "dataset:\n",
        "    The dataset object passed during initialization.\n",
        "batch_size:\n",
        "    Number of samples per batch.\n",
        "shuffle:\n",
        "    Indicates if the data should be shuffled.\n",
        "num_workers:\n",
        "    Number of subprocesses used for data loading.\n",
        "collate_fn:\n",
        "    Function used for batching data.\n",
        "sampler:\n",
        "    Custom sampler for determining the order of data loading.\n",
        "indices:\n",
        "    The list of data indices used for batching, which can be shuffled.\n",
        "label_to_idx:\n",
        "    Dictionary mapping car type names (or other dataset labels) to class indices.\n",
        "drop_last:\n",
        "    Whether to drop the last incomplete batch.\n",
        "pin_memory:\n",
        "    Whether to enable pinned memory for CUDA operations.\n",
        "pin_memory_device:\n",
        "    The device used for pinning memory if applicable.\n",
        "\n",
        "Usage:\n",
        "------\n",
        "This DataLoader can be used in place of PyTorch's `DataLoader` for custom batching and preprocessing.\n",
        "Example:\n",
        "    >>> dataloader = CarDataLoader(car_dataset, batch_size=64, shuffle=True)\n",
        "    >>> for images, labels in dataloader:\n",
        "    >>>     # Use images and labels in training\n",
        "\"\"\"\n",
        "class CarDataLoader:\n",
        "    def __init__(self, dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=None, sampler=None, drop_last=False, pin_memory=False, timeout=0, worker_init_fn=None, prefetch_factor=None, persistent_workers=False, pin_memory_device=''):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.collate_fn = collate_fn if collate_fn else self.default_collate_fn\n",
        "        self.sampler = sampler\n",
        "        self.drop_last = drop_last\n",
        "        self.pin_memory = pin_memory\n",
        "        self.timeout = timeout\n",
        "        self.worker_init_fn = worker_init_fn\n",
        "        self.prefetch_factor = prefetch_factor\n",
        "        self.persistent_workers = persistent_workers\n",
        "        self.pin_memory_device = pin_memory_device\n",
        "        self.label_to_idx = dataset.class_to_idx if isinstance(dataset, CarDataset) else dataset.dataset.class_to_idx\n",
        "\n",
        "        # If no sampler is provided, use a simple sequential sampler\n",
        "        if self.sampler is None:\n",
        "            self.indices = list(range(len(dataset)))\n",
        "            if self.shuffle:\n",
        "                random.shuffle(self.indices)\n",
        "        else:\n",
        "            self.indices = list(self.sampler)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Calculate number of batches\n",
        "        if self.drop_last:\n",
        "            return len(self.indices) // self.batch_size\n",
        "        else:\n",
        "            return (len(self.indices) + self.batch_size - 1) // self.batch_size  # Integer division with rounding up\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Shuffle the indices only if no sampler is used\n",
        "        if self.sampler is None and self.shuffle:\n",
        "            random.shuffle(self.indices)\n",
        "\n",
        "        # Generate batches using the indices from the sampler or the list\n",
        "        for i in range(0, len(self.indices), self.batch_size):\n",
        "            batch_indices = self.indices[i:i + self.batch_size]\n",
        "\n",
        "            # Handle the case where the last batch may be smaller if drop_last is False\n",
        "            if self.drop_last and len(batch_indices) < self.batch_size:\n",
        "                continue\n",
        "\n",
        "            batch_data = [self.dataset[idx] for idx in batch_indices]\n",
        "\n",
        "            # Apply the collate function to batch the data\n",
        "            yield self.collate_fn(batch_data)\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch_data):\n",
        "        images, labels = zip(*batch_data)\n",
        "        images = torch.stack(images)  # Stack images into a batch\n",
        "        labels = torch.tensor(labels)  # Convert labels to a tensor\n",
        "        return images, labels\n",
        "\n",
        "\n",
        "    def default_collate_fn(self, batch_data):\n",
        "        \"\"\"Default collate function that stacks images and labels into batches.\"\"\"\n",
        "        images, labels = zip(*batch_data)\n",
        "\n",
        "        images = torch.stack(images)  # Stack images into a batch\n",
        "        labels = torch.tensor(labels)  # Convert labels to a tensor\n",
        "\n",
        "        if self.pin_memory:\n",
        "          if torch.cuda.is_available():\n",
        "            images = images.pin_memory()\n",
        "\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxXqdlRYOnH"
      },
      "source": [
        "##test datalodar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8uhM7CaYSKk"
      },
      "outputs": [],
      "source": [
        "train_data_ledor = CarDataLoader(train_data, batch_size=32, shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "print(f\"Number of batches: {len(train_data_ledor)}\")\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(train_data_ledor):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\" - Images shape: {images.shape}\")\n",
        "    print(f\" - Labels shape: {labels.shape}\")\n",
        "    if batch_idx >= 1:\n",
        "        break\n",
        "\n",
        "batch_data = [train_data[idx] for idx in range(5)]\n",
        "images, labels = train_data_ledor.collate_fn(batch_data)\n",
        "\n",
        "print(f\"Images shape from collate_fn: {images.shape}\")\n",
        "print(f\"Labels from collate_fn: {labels.shape}\")\n",
        "train_data_ledor.__len__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8soRNJiQYbtd"
      },
      "source": [
        "#CNN Nerworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xPzFKPqzho9"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet34(num_classes=1000):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "441MDEnbyTIV"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CifarCNN(nn.Module):\n",
        "    \"\"\"CNN for the CIFAR Dataset with 196 classes\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=196):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CifarCNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),  # LeakyReLU במקום ReLU\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 2 = 224 / 2 = 112\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 4 = 112 / 2 = 56\n",
        "            nn.Dropout2d(p=0.2),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # input_resolution / 8 = 56 / 2 = 28\n",
        "        )\n",
        "\n",
        "        # Update the input size of the fully connected layer\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(256 * 16 * 16, 1024),  # Update based on output dimensions after conv layers\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(inplace=True, negative_slope=0.01),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes)  # Final output layer for 196 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward pass.\"\"\"\n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "\n",
        "        # fully connected layers\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiv75zMMVn7k"
      },
      "outputs": [],
      "source": [
        "class CarCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CarCNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # Compute the size after the convolutional layers\n",
        "        self._initialize_fc_layer()\n",
        "\n",
        "    def _initialize_fc_layer(self):\n",
        "        # Pass a dummy tensor through the conv layers to calculate the output size\n",
        "        dummy_input = torch.zeros(1, 3, 128, 128)  # Example input: batch_size=1, 3 channels, 128x128 images\n",
        "        output = self.conv_layer(dummy_input)\n",
        "        flattened_size = output.view(1, -1).size(1)  # Flatten and get the size\n",
        "        # Fully connected layers\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512 * 16 * 16, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing to run the model"
      ],
      "metadata": {
        "id": "EwWBa1TqGumM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU-HNO9WMbbV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZFXmrpiGown"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##smaller data"
      ],
      "metadata": {
        "id": "2evHoZJFG0QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imited_annotations_10_train = merged_data_train.groupby('class_id').apply(lambda x: x.sample(min(len(x), 10))).reset_index(drop=True)\n",
        "excluded_class_id = 196\n",
        "filtered_annotations_train = imited_annotations_10_train[imited_annotations_10_train['class_id'] != excluded_class_id]\n",
        "imited_annotations_10_test = merged_data_test.groupby('class_id').apply(lambda x: x.sample(min(len(x), 10))).reset_index(drop=True)\n",
        "filtered_annotations_test = imited_annotations_10_test[imited_annotations_10_test['class_id'] != excluded_class_id]\n",
        "filtered_annotations_test\n",
        "results_df = pd.DataFrame(columns=[\"Model\", \"Test Accuracy\", \"Test Loss\", \"Training Time (s)\"])"
      ],
      "metadata": {
        "id": "nrjV3NKzRKzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cutting in to 195 different categories"
      ],
      "metadata": {
        "id": "cq0FFmr-G5Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_class_id = 196\n",
        "train_whit195 = merged_data_train[merged_data_train['class_id'] != excluded_class_id]\n",
        "test_whit195 = merged_data_test[merged_data_test['class_id'] != excluded_class_id]\n",
        "test_whit195"
      ],
      "metadata": {
        "id": "Rh4szauCkV9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY16_FvqZr5t"
      },
      "source": [
        "#First run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBtJ-WR0DHew"
      },
      "outputs": [],
      "source": [
        "def log_final_results(model_name, test_accuracy, test_loss, training_time, results_df):\n",
        "    \"\"\"\n",
        "    Adds the final results of a model run into the global DataFrame.\n",
        "    \"\"\"\n",
        "    new_row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Test Accuracy\": test_accuracy,\n",
        "        \"Test Loss\": test_loss,\n",
        "        \"Training Time (s)\": training_time\n",
        "    }\n",
        "\n",
        "    new_df = pd.DataFrame([new_row])\n",
        "    results_df = pd.concat([results_df, new_df], ignore_index=True)\n",
        "\n",
        "    # Check if the file already exists\n",
        "    try:\n",
        "        # Try to read the existing CSV file\n",
        "        existing_df = pd.read_csv(\"all_model_results.csv\")\n",
        "        results_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
        "    except FileNotFoundError:\n",
        "        # If the file doesn't exist, don't do anything\n",
        "        pass\n",
        "\n",
        "    # Save the updated DataFrame to the CSV file\n",
        "    results_df.to_csv(\"all_model_results.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_dl, test_dl, loss_fn, optimizer, scheduler, n_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    model.train()  # Set the model to train mode initially\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm.tqdm(range(1, n_epochs+1)):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_dl, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels - 1\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=-1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (labels == predicted).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dl.dataset)\n",
        "        epoch_acc = running_correct / len(train_dl.dataset) * 100.0\n",
        "\n",
        "        logging.info(f\"Epoch {epoch}/{n_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}%\")\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()  # Switch to eval mode to evaluate on test data\n",
        "        test_loss, test_acc = eval_model(model, test_dl, loss_fn)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            logging.info('Model saved.')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        model.train()  # Switch back to train mode after validation\n",
        "        scheduler.step(test_acc)  # Step the scheduler\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model, losses, accuracies, test_losses, test_accuracies\n",
        "\n",
        "\n",
        "def eval_model(model, test_dl, loss_fn):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    loss_total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dl:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels - 1\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss_total += loss.item()\n",
        "            predicted = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = 100.0 * correct / total\n",
        "    test_loss = loss_total / len(test_dl.dataset)\n",
        "\n",
        "    logging.info(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}%')\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def plot_metrics(metrics, title, xlabel, ylabel, legend_labels, save_path=None):\n",
        "    \"\"\"\n",
        "    Helper function to plot metrics.\n",
        "    \"\"\"\n",
        "    for metric, label in zip(metrics, legend_labels):\n",
        "        plt.plot(metric, label=label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_on_test(model, test_data, criterion, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset and plot the results.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained model to evaluate.\n",
        "        test_data (Dataset): Dataset for testing.\n",
        "        criterion (Loss): Loss function to compute test loss.\n",
        "        device (torch.device): Device to run computations (CPU or GPU).\n",
        "        batch_size (int): Batch size for the CarDataLoader.\n",
        "\n",
        "    Returns:\n",
        "        float, float: Final test loss and accuracy.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    # Create DataLoader\n",
        "    test_loader = CarDataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "\n",
        "            # Move to device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Adjust labels if necessary\n",
        "            labels = labels - 1\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_test_loss = test_loss / total\n",
        "    accuracy = correct / total\n",
        "    test_losses.append(avg_test_loss)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Plotting results\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(test_losses) + 1), test_losses, marker='o', label='Test Loss')\n",
        "    plt.plot(range(1, len(accuracies) + 1), [acc * 100 for acc in accuracies], marker='s', label='Test Accuracy')\n",
        "    plt.title(\"Test Loss and Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return avg_test_loss, accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Model, optimizer, and loss function setup\n",
        "    global results_df\n",
        "    model_name = \"CifarCNN_195\"\n",
        "    transform=transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 224x224 for uniformity\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = CarDataset(train_whit195, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "    num_classes = 195\n",
        "    results_table = []\n",
        "    model = CifarCNN(num_classes)\n",
        "    optimizer = Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross Validation setup\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    start_time = time.time()\n",
        "    # Device setup (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    num_epochs = 10\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # K-fold loop\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "        # Subset the dataset for training and validation\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = CarDataLoader(train_subsampler, batch_size=32, shuffle=True, num_workers=4)\n",
        "        val_loader = CarDataLoader(val_subsampler, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "        # Reset model, optimizer, and loss function for each fold\n",
        "        model = CifarCNN(num_classes=num_classes).to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
        "            model, train_loader, val_loader, loss_fn, optimizer, scheduler, n_epochs=num_epochs\n",
        "        )\n",
        "        results_table.append({\n",
        "            \"Model Name\": model_name,\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Train Loss\": train_losses[-1],\n",
        "            \"Validation Loss\": val_losses[-1],\n",
        "            \"Validation Accuracy\": val_accuracies[-1],\n",
        "            \"Test Loss\": None,\n",
        "            \"Test Accuracy\": None\n",
        "        })\n",
        "\n",
        "        # Store results\n",
        "        results[fold] = {'loss': val_losses[-1], 'accuracy': val_accuracies[-1]}\n",
        "        all_train_losses.append(train_losses)\n",
        "        all_val_losses.append(val_losses)\n",
        "        all_val_accuracies.append(val_accuracies)\n",
        "    # Calculate average performance across all folds\n",
        "    avg_loss = sum([results[fold]['loss'] for fold in results]) / k_folds\n",
        "    avg_accuracy = sum([results[fold]['accuracy'] for fold in results]) / k_folds\n",
        "\n",
        "    print(f'\\nAverage Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_train_losses[fold], all_val_losses[fold]],\n",
        "            title=f'Fold {fold+1} Loss',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Loss',\n",
        "            legend_labels=['Train Loss', 'Validation Loss']\n",
        "        )\n",
        "\n",
        "    # 2. Validation accuracy per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_val_accuracies[fold]],\n",
        "            title=f'Fold {fold+1} Validation Accuracy',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Accuracy (%)',\n",
        "            legend_labels=['Validation Accuracy']\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    total_training_time = end_time - start_time\n",
        "    test_dataset = CarDataset(test_whit195, '/content/stanford_cars/car_data/car_data/test', transform=transform)\n",
        "    test_loss, test_accuracy = evaluate_model_on_test(model, test_dataset, loss_fn, device)\n",
        "    log_final_results(model_name, test_accuracy, test_loss, total_training_time, results_df)\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    model_save_path = f\"{model_name}_final_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "HPtEN8N8zrbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"results_checkpoint.csv\"):\n",
        "    results_df = pd.read_csv(\"results_checkpoint.csv\")\n",
        "else:\n",
        "    results_df = pd.DataFrame(columns=[\"Model Name\", \"Fold\", \"Train Loss\", \"Validation Loss\", \"Validation Accuracy\", \"Test Loss\", \"Test Accuracy\"])\n"
      ],
      "metadata": {
        "id": "2yCiXcT0fF3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tT2tM1QBpU-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "-2TuPVF25DGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "NhaGHvB3blY7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IjMthdWYvi1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Model, optimizer, and loss function setup\n",
        "    global results_df\n",
        "    model_name = \"CifarCNN_improv1_195\"\n",
        "    transform=transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 224x224 for uniformity\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = CarDataset(train_whit195, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "    num_classes = 195\n",
        "    results_table = []\n",
        "    model = CifarCNN(num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "    # K-Fold Cross Validation setup\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    start_time = time.time()\n",
        "    # Device setup (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    num_epochs = 10\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # K-fold loop\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "        # Subset the dataset for training and validation\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = CarDataLoader(train_subsampler, batch_size=32, shuffle=True, num_workers=2)\n",
        "        val_loader = CarDataLoader(val_subsampler, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "        # Reset model, optimizer, and loss function for each fold\n",
        "        model = CifarCNN(num_classes=num_classes).to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
        "            model, train_loader, val_loader, loss_fn, optimizer, scheduler, n_epochs=num_epochs\n",
        "        )\n",
        "        results_table.append({\n",
        "            \"Model Name\": model_name,\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Train Loss\": train_losses[-1],\n",
        "            \"Validation Loss\": val_losses[-1],\n",
        "            \"Validation Accuracy\": val_accuracies[-1],\n",
        "            \"Test Loss\": None,\n",
        "            \"Test Accuracy\": None\n",
        "        })\n",
        "\n",
        "        # Store results\n",
        "        results[fold] = {'loss': val_losses[-1], 'accuracy': val_accuracies[-1]}\n",
        "        all_train_losses.append(train_losses)\n",
        "        all_val_losses.append(val_losses)\n",
        "        all_val_accuracies.append(val_accuracies)\n",
        "        scheduler.step(val_losses[-1])\n",
        "        print(f\"Fold {fold + 1}/{k_folds} - Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
        "        pd.DataFrame(results_table).to_csv(\"partial_results.csv\", index=False)\n",
        "\n",
        "    # Calculate average performance across all folds\n",
        "    avg_loss = sum([results[fold]['loss'] for fold in results]) / k_folds\n",
        "    avg_accuracy = sum([results[fold]['accuracy'] for fold in results]) / k_folds\n",
        "\n",
        "    print(f'\\nAverage Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_train_losses[fold], all_val_losses[fold]],\n",
        "            title=f'Fold {fold+1} Loss',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Loss',\n",
        "            legend_labels=['Train Loss', 'Validation Loss']\n",
        "        )\n",
        "\n",
        "    # 2. Validation accuracy per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_val_accuracies[fold]],\n",
        "            title=f'Fold {fold+1} Validation Accuracy',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Accuracy (%)',\n",
        "            legend_labels=['Validation Accuracy']\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    total_training_time = end_time - start_time\n",
        "    test_dataset = CarDataset(test_whit195, '/content/stanford_cars/car_data/car_data/test', transform=transform)\n",
        "    test_loss, test_accuracy = evaluate_model_on_test(model, test_dataset, loss_fn, device)\n",
        "    log_final_results(model_name, test_accuracy, test_loss, total_training_time, results_df)\n",
        "    model_save_path = f\"{model_name}_final_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "lfCqIKTLpTu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Clipping"
      ],
      "metadata": {
        "id": "CJeNvvfqgEVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dl, test_dl, loss_fn, optimizer, scheduler, n_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    model.train()  # Set the model to train mode initially\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm.tqdm(range(1, n_epochs+1)):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_dl, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels - 1  # Adjust labels if needed\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=-1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (labels == predicted).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dl.dataset)\n",
        "        epoch_acc = running_correct / len(train_dl.dataset) * 100.0\n",
        "\n",
        "        logging.info(f\"Epoch {epoch}/{n_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}%\")\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()  # Switch to eval mode to evaluate on test data\n",
        "        test_loss, test_acc = eval_model(model, test_dl, loss_fn)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            logging.info('Model saved.')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        model.train()  # Switch back to train mode after validation\n",
        "        scheduler.step(test_acc)  # Step the scheduler\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model, losses, accuracies, test_losses, test_accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Model, optimizer, and loss function setup\n",
        "    global results_df\n",
        "    model_name = \"CifarCNN_improv2_195\"\n",
        "    transform=transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 224x224 for uniformity\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = CarDataset(train_whit195, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "    num_classes = 195\n",
        "    results_table = []\n",
        "    model = CifarCNN(num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "    # K-Fold Cross Validation setup\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    start_time = time.time()\n",
        "    # Device setup (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    num_epochs = 10\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # K-fold loop\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "        # Subset the dataset for training and validation\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = CarDataLoader(train_subsampler, batch_size=32, shuffle=True, num_workers=4)\n",
        "        val_loader = CarDataLoader(val_subsampler, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "        # Reset model, optimizer, and loss function for each fold\n",
        "        model = CifarCNN(num_classes=num_classes).to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
        "            model, train_loader, val_loader, loss_fn, optimizer, scheduler, n_epochs=num_epochs\n",
        "        )\n",
        "        results_table.append({\n",
        "            \"Model Name\": model_name,\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Train Loss\": train_losses[-1],\n",
        "            \"Validation Loss\": val_losses[-1],\n",
        "            \"Validation Accuracy\": val_accuracies[-1],\n",
        "            \"Test Loss\": None,\n",
        "            \"Test Accuracy\": None\n",
        "        })\n",
        "\n",
        "        # Store results\n",
        "        results[fold] = {'loss': val_losses[-1], 'accuracy': val_accuracies[-1]}\n",
        "        all_train_losses.append(train_losses)\n",
        "        all_val_losses.append(val_losses)\n",
        "        all_val_accuracies.append(val_accuracies)\n",
        "        scheduler.step(val_losses[-1])\n",
        "    # Calculate average performance across all folds\n",
        "    avg_loss = sum([results[fold]['loss'] for fold in results]) / k_folds\n",
        "    avg_accuracy = sum([results[fold]['accuracy'] for fold in results]) / k_folds\n",
        "\n",
        "    print(f'\\nAverage Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_train_losses[fold], all_val_losses[fold]],\n",
        "            title=f'Fold {fold+1} Loss',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Loss',\n",
        "            legend_labels=['Train Loss', 'Validation Loss']\n",
        "        )\n",
        "\n",
        "    # 2. Validation accuracy per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_val_accuracies[fold]],\n",
        "            title=f'Fold {fold+1} Validation Accuracy',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Accuracy (%)',\n",
        "            legend_labels=['Validation Accuracy']\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    total_training_time = end_time - start_time\n",
        "    test_dataset = CarDataset(test_whit195, '/content/stanford_cars/car_data/car_data/test', transform=transform)\n",
        "    test_loss, test_accuracy = evaluate_model_on_test(model, test_dataset, loss_fn, device)\n",
        "    log_final_results(model_name, test_accuracy, test_loss, total_training_time, results_df)\n",
        "    model_save_path = f\"{model_name}_final_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AOLj4t9NgIVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()#GPT ram cleanup"
      ],
      "metadata": {
        "id": "-MGrudSkInFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#d"
      ],
      "metadata": {
        "id": "ZckW4KDx-sf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dl, test_dl, loss_fn, optimizer, scheduler, n_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    model.train()  # Set the model to train mode initially\n",
        "    best_test_acc = 0\n",
        "    for epoch in tqdm.tqdm(range(1, n_epochs+1)):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_dl, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels - 1  # Adjust labels if needed\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, dim=-1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (labels == predicted).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dl.dataset)\n",
        "        epoch_acc = running_correct / len(train_dl.dataset) * 100.0\n",
        "\n",
        "        logging.info(f\"Epoch {epoch}/{n_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}%\")\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()  # Switch to eval mode to evaluate on test data\n",
        "        test_loss, test_acc = eval_model(model, test_dl, loss_fn)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            logging.info('Model saved.')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        model.train()  # Switch back to train mode after validation\n",
        "        scheduler.step(test_acc)  # Step the scheduler\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model, losses, accuracies, test_losses, test_accuracies\n",
        "\n",
        "def inference_with_augmentation(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Collect predictions from multiple augmented versions of the image\n",
        "            augmented_preds = []\n",
        "            for _ in range(5):  # You can modify the number of augmentations\n",
        "                augmented_images = apply_augmentation(images).to(device)  # Implement a function to apply augmentation\n",
        "                outputs = model(augmented_images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                augmented_preds.append(preds)\n",
        "\n",
        "            # Aggregate predictions (majority voting)\n",
        "            augmented_preds = torch.stack(augmented_preds, dim=0)\n",
        "            final_preds = torch.mode(augmented_preds, dim=0).values  # Majority vote\n",
        "\n",
        "            predictions.append(final_preds)\n",
        "            targets.append(labels)\n",
        "\n",
        "    predictions = torch.cat(predictions, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predictions == targets).sum().item() / len(targets)\n",
        "    return accuracy\n",
        "\n",
        "def apply_augmentation(images):\n",
        "    # Convert tensors back to PIL Images for augmentation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),  # Convert tensor to PIL Image\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "        transforms.ToTensor(),  # Convert back to tensor\n",
        "    ])\n",
        "\n",
        "    # Apply augmentation for each image in the batch\n",
        "    augmented_images = torch.stack([transform(image) for image in images])\n",
        "    return augmented_images\n",
        "\n",
        "def main():\n",
        "    # Model, optimizer, and loss function setup\n",
        "    global results_df\n",
        "    model_name = \"CifarCNN_195_augmentation\"\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((128, 128)),  # Resize to 128x128 for uniformity\n",
        "        transforms.ToTensor(),  # Convert to Tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "    ])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = CarDataset(train_whit195, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "    num_classes = 195\n",
        "    results_table = []\n",
        "\n",
        "    model = CifarCNN(num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # K-Fold Cross Validation setup\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    num_epochs = 10\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # K-fold loop\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "        # Subset the dataset for training and validation\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = CarDataLoader(train_subsampler, batch_size=32, shuffle=True, num_workers=4)\n",
        "        val_loader = CarDataLoader(val_subsampler, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "        # Reset model, optimizer, and loss function for each fold\n",
        "        model = CifarCNN(num_classes=num_classes).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "        # Learning rate scheduler\n",
        "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
        "            model, train_loader, val_loader, loss_fn, optimizer, scheduler, n_epochs=num_epochs\n",
        "        )\n",
        "\n",
        "        # Inference-Time Augmentation (TTA) during validation\n",
        "        val_accuracy_with_tta = inference_with_augmentation(model, val_loader, device)\n",
        "\n",
        "        results_table.append({\n",
        "            \"Model Name\": model_name,\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Train Loss\": train_losses[-1],\n",
        "            \"Validation Loss\": val_losses[-1],\n",
        "            \"Validation Accuracy\": val_accuracies[-1],\n",
        "            \"Test Loss\": None,\n",
        "            \"Test Accuracy\": None,\n",
        "            \"Validation Accuracy with TTA\": val_accuracy_with_tta  # Add augmented accuracy\n",
        "        })\n",
        "\n",
        "        # Store results\n",
        "        results[fold] = {'loss': val_losses[-1], 'accuracy': val_accuracies[-1], 'accuracy_with_tta': val_accuracy_with_tta}\n",
        "        all_train_losses.append(train_losses)\n",
        "        all_val_losses.append(val_losses)\n",
        "        all_val_accuracies.append(val_accuracies)\n",
        "        scheduler.step(val_losses[-1])\n",
        "        print(f\"Fold {fold + 1}/{k_folds} - Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
        "    # Calculate average performance across folds with TTA\n",
        "    avg_loss = sum([results[fold]['loss'] for fold in results]) / k_folds\n",
        "    avg_accuracy = sum([results[fold]['accuracy'] for fold in results]) / k_folds\n",
        "    avg_accuracy_with_tta = sum([results[fold]['accuracy_with_tta'] for fold in results]) / k_folds\n",
        "\n",
        "    print(f'\\nAverage Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')\n",
        "    print(f'Average Accuracy with TTA: {avg_accuracy_with_tta:.4f}')\n",
        "\n",
        "    # Plot Loss per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_train_losses[fold], all_val_losses[fold]],\n",
        "            title=f'Fold {fold+1} Loss',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Loss',\n",
        "            legend_labels=['Train Loss', 'Validation Loss']\n",
        "        )\n",
        "\n",
        "    # Plot Validation Accuracy per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_val_accuracies[fold]],\n",
        "            title=f'Fold {fold+1} Validation Accuracy',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Accuracy (%)',\n",
        "            legend_labels=['Validation Accuracy']\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    total_training_time = end_time - start_time\n",
        "    test_dataset = CarDataset(test_whit195, '/content/stanford_cars/car_data/car_data/test', transform=transform)\n",
        "    test_loss, test_accuracy = evaluate_model_on_test(model, test_dataset, loss_fn, device)\n",
        "    log_final_results(model_name, test_accuracy, test_loss, total_training_time, results_df)\n",
        "    model_save_path = f\"{model_name}_final_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "sjbj2unG-thn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#new category"
      ],
      "metadata": {
        "id": "BHQ9aqgqj8gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Model, optimizer, and loss function setup\n",
        "    global results_df\n",
        "    model_name = \"CifarCNN_new_catgari_196\"\n",
        "    transform=transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize to 224x224 for uniformity\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = CarDataset(merged_data_train, '/content/stanford_cars/car_data/car_data/train', transform=transform)\n",
        "    num_classes = 196\n",
        "    results_table = []\n",
        "    model = CifarCNN(num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "    # K-Fold Cross Validation setup\n",
        "    k_folds = 5\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    start_time = time.time()\n",
        "    # Device setup (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    num_epochs = 10\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # K-fold loop\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "        # Subset the dataset for training and validation\n",
        "        train_subsampler = Subset(dataset, train_ids)\n",
        "        val_subsampler = Subset(dataset, val_ids)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = CarDataLoader(train_subsampler, batch_size=32, shuffle=True, num_workers=4)\n",
        "        val_loader = CarDataLoader(val_subsampler, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "        # Reset model, optimizer, and loss function for each fold\n",
        "        model = CifarCNN(num_classes=num_classes).to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
        "            model, train_loader, val_loader, loss_fn, optimizer, scheduler, n_epochs=num_epochs\n",
        "        )\n",
        "        results_table.append({\n",
        "            \"Model Name\": model_name,\n",
        "            \"Fold\": fold + 1,\n",
        "            \"Train Loss\": train_losses[-1],\n",
        "            \"Validation Loss\": val_losses[-1],\n",
        "            \"Validation Accuracy\": val_accuracies[-1],\n",
        "            \"Test Loss\": None,\n",
        "            \"Test Accuracy\": None\n",
        "        })\n",
        "\n",
        "        # Store results\n",
        "        results[fold] = {'loss': val_losses[-1], 'accuracy': val_accuracies[-1]}\n",
        "        all_train_losses.append(train_losses)\n",
        "        all_val_losses.append(val_losses)\n",
        "        all_val_accuracies.append(val_accuracies)\n",
        "        scheduler.step(val_losses[-1])\n",
        "    # Calculate average performance across all folds\n",
        "    avg_loss = sum([results[fold]['loss'] for fold in results]) / k_folds\n",
        "    avg_accuracy = sum([results[fold]['accuracy'] for fold in results]) / k_folds\n",
        "\n",
        "    print(f'\\nAverage Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_train_losses[fold], all_val_losses[fold]],\n",
        "            title=f'Fold {fold+1} Loss',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Loss',\n",
        "            legend_labels=['Train Loss', 'Validation Loss']\n",
        "        )\n",
        "\n",
        "    # 2. Validation accuracy per fold\n",
        "    for fold in range(k_folds):\n",
        "        plot_metrics(\n",
        "            [all_val_accuracies[fold]],\n",
        "            title=f'Fold {fold+1} Validation Accuracy',\n",
        "            xlabel='Epochs',\n",
        "            ylabel='Accuracy (%)',\n",
        "            legend_labels=['Validation Accuracy']\n",
        "        )\n",
        "    end_time = time.time()\n",
        "    total_training_time = end_time - start_time\n",
        "    test_dataset = CarDataset(test_whit195, '/content/stanford_cars/car_data/car_data/test', transform=transform)\n",
        "    test_loss, test_accuracy = evaluate_model_on_test(model, test_dataset, loss_fn, device)\n",
        "    log_final_results(model_name, test_accuracy, test_loss, total_training_time, results_df)\n",
        "    model_save_path = f\"{model_name}_final_model.pth\"\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n",
        "    results_df = pd.DataFrame(results_table)\n",
        "    print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Nn9emQBMj79y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ThxXqdlRYOnH"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}